{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux de neurones seq2seq sur des séries temporelles\n",
    "\n",
    "Le plan de match est d'abord d'entraîner un tel réseau sur un tout petit dataset de 2 signaux sinusoidaux simples, puis de tenter de prédire des signaux de plus en plus complexes et de natures un peu différente. \n",
    "\n",
    "## Comment utiliser ce notebook au format \".ipynb\" ?\n",
    "\n",
    "Mis à part les versions exportées et le code Python mis à disposition, il est plus intéressant de rouler le code dans un Jupyter notebook pour avoir une approche itérative. Dans un tel notebook, il est possible de rouler chaque cellule une par une (`CTRL+ENTER` ou avec `SHIFT+ENTER`) ou aussi de rouler à nouveau toutes les cellules (cliquer dans les menus sur `Kernel>Restart` pour recommencer, et puis faire `Cell>Run all`). \n",
    "\n",
    "Pour installer `jupyter`, il est possible de rouler la commande suivante : \"`pip install jupyter`\", cependant d'autres méthodes d'installation sont préférables si disponibles (ex: une commande `sudo apt-get`). \n",
    "Il se peut qu'il soit requis de faire \"`pip install --upgrade pip`\" dépendamment de votre installation actuelle. \n",
    "\n",
    "Une fois jupyter correctement installé, il est possible d'ouvrir ce notebook en ouvrant le dossier dans lequel il se trouve en ligne de commande, puis en roulant la commande \"`jupyter notebook`\". Dépendemment de votre installation, il se peut que vous devez plutôt rouler la commande \"`ipython notebook`\", et cela potentiellement en mettant une lettre majuscule à iPython, tel que la commande \"`iPython notebook`\".\n",
    "\n",
    "## Exercices\n",
    "\n",
    "Note: le dataset change en fonction de l'exercice, et en conséquence, la structure du réseau de neurone changera aussi pour s'adapter, de façon automatique selon la section `Paramètres du réseau de neuronne` plus bas. Il restera au participant de modifier d'avantage ces paramètres afin de réellement pouvoir prédire le signal correctement. \n",
    "\n",
    "### Exercice 1\n",
    "\n",
    "En théorie, il est possible pour cet exercice de créer une estimation parfaite du signal (excepté certaines erreurs d'arrondi et différences minimes). Cet exercice est surtout pour s'approprier le code et explorer le fonctionnement de seq2seq au niveau des dimensions et du lien avec le code avec TensorFlow avant de passer aux exercices suivants. \n",
    "\n",
    "Nous avons 2 séries temporelles différentes en parallèle à prédire: notre réseau de neurones est multidimensionnel car il prédit plusieurs séries à la fois. Ainsi, à chaque étape de temps, un encodeur accepte en argument 2 valeurs de \"hauteur\" d'un signal. Ici, pour l'exercice 1, c'est simple: on a un sinus et son cosinus en entrée dans l'encodeur pour 10 étapes de temps, et il faut prédire les 10 prochaines étapes de temps par le biais du décodeur. Plus de détails sur ce signal se trouvent dans le fichier `datasets.py` où la fonction pour ce dataset est définie. \n",
    "\n",
    "L'exercice 1 devrait fonctionner du premier coup avec le code actuel. Le code a été fait de façon à être améliorable afin de mieux résoudre cet exercice, quoiqu'il est possible de passer à l'exercice 2 directement aussi. Voici juste par exemple le résultat que vous devriez obtenir avec les paramètres déjà fourni, mais il est possible de faire bien mieux que cela : \n",
    "\n",
    "<img src=\"images/E1.png\" />\n",
    "\n",
    "\n",
    "### Exercice 2\n",
    "\n",
    "Ici on a un seul signal à prédire plutôt que 2 en même temps, cependant ce signal est plus compliqué à prédire. C'est une combinaison 2 fréquences, et ces fréquences peuvent varier librement dans une intervalle de fréquence pré-établie. De plus, ces séquences sont plus longues comparativement à ceux de l'exercice 1. \n",
    "\n",
    "Pour réussir cet exercice (et les suivants) avec des prédictions du moins presque correctes à l'oeil, il sera nécessaire de modifier les paramètres du réseau de neurones. Indice: augmenter le nombre de neurones, le nombre d'étapes d'entrainement, revoir les hyperparamètres de l'optimisation (ex: taux d'apprentissage), augmenter de couches de neurones empilées, etc. Par exemple, il est possible d'obtenir cette prédiction avec un `nb_iters = 2500`, un `batch_size = 50` et un `hidden_dim = 35`, mais il est possible de résoudre cela de façon plus efficace avec d'autres valeurs : \n",
    "\n",
    "<img src=\"images/E2.png\" />\n",
    "\n",
    "Rappel : Le réseau de neurones ici ne voit seulement que la partie gauche du graphique et ne voit aucune données de plus que ceux qui sont visibles pour effectuer cette prédiction. \n",
    "\n",
    "### Exercice 3\n",
    "\n",
    "Cet exercice ressemble beaucoup à l'exercice 2. Cependant, en plus du fait que les longeurs d'onde peuvent varier dans ce signal, j'y ai aussi ajouté un peu de bruit: des oscillations hasardeuses néfastes. Le but est donc de prédire la suite de ce signal, et cette prédiction doit faire comme si le bruit néfaste n'existait pas afin de faire une prédiction lisse. Les Y fournis seront, en conséquence, lisses, comparativement aux X. Cela sera spécialement visible dans les graphiques finaux suite aux prédictions. \n",
    "\n",
    "La suggestion suivante est audacieuse et est pour les motivés: il est possible d'utiliser des cellules récurrentes différentes pour le décodeur et l'encodeur, et aussi il est possible d'ajouter du feedback dans le décodeur (envoyer les prédictions en tant qu'entrée suivante dans le temps). Cependant, je ne m'attends pas à ce qu'un débutant ne fasse cela en seulement une heure. Pour réaliser cela, il faudrait modifier l'appel à `tf.nn.seq2seq.basic_rnn_seq2seq` et le code autour de cela dans le code actuel pour utiliser d'autres fonctions du module `tf.nn.seq2seq` ou sinon en appelant les deux cellules de rnn (un décodeur et un encodeur) manuellement avec la méthode `__call__` qui a pour rôle de boucler sur les étapes de temps manuellement. Plus d'informations à : https://www.tensorflow.org/versions/r0.12/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods\n",
    "\n",
    "### Exercice 4\n",
    "\n",
    "Pour l'exercice 3, le problème est beaucoup plus difficile que pour les exercices précédents. Il s'agit de prédire les valeures futures pour le Bitcoin. Comme pour l'exercice 1, on a 2 signaux en entrées et aussi 2 signaux en sortie: ce sont les valeurs BTC/USD et BTC/EUR au travers du temps, soit des historiques de valeurs du Bitcoin par rapport au dollar américain et au euro. Le Bitcoin est une cryopto-monnaie et n'est pas une monnaie officielle, mais cela ne l'empêche pas d'avoir une valeur qui varie pour faire des échanges, au même titre que les monnaies normales. \n",
    "\n",
    "En réalité, il faudrait utiliser des données beaucoup plus abondantes et massives (par exemple, un historique de prix aux minutes plutôt qu'aux jours). Présentement, il serait très difficile de créer un prédicteur correct à cause des limitations actuelles sur le jeu de données fourni, mais le problème reste intéressant. \n",
    "\n",
    "Des ajouts possibles aux deux dimensions d'entrées (BTC/USD et BTC/EUR) seraient de créer des dimensions supplémentaires contenant des ondes sinusoidales (ou des ondes triangulaires ou en scie, etc) qui cyclent avec les heures, les jours, les mois, les années et les lunes, etc. Il serait aussi intéressant d'obtenir d'autres signaux, tels que des signaux de température sur plusieurs dimensions différentes pour différents emplacements dans le monde, etc. Beaucoup de sources de données pourraient être aggrégées et combinées avec d'autres données financières. D'autres données fiancières pourraient être très utiles, par exemple, le Dow Jones et le S&P 500.\n",
    "\n",
    "Avec les exemples mentionnés, il serait possible d'avoir des données en entrées à N dimensions (BTC/USD, BTC/EUR, Dow_Jones, SP_500, heures, jours, semaines, mois, années, lunes, meteo_USA, meteo_EUROPE, meteo_ASIE), et 2 dimensions de sortie pour les prédictions (BTC/USD, BTC/EUR). \n",
    "\n",
    "Ces concepts peuvent s'appliquer à plusieurs problèmes. Par exemple, actuellement les réseaux de neurones sont présentement utilisés pour faire des prédiction météorologiques. \n",
    "\n",
    "## Pour décider de l'exercice à faire, changer la valeur de la variable suivante, nommée \"exercice\" :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exercice = 3\n",
    "# Valeurs possibles : 1, 2, ou 3. \n",
    "\n",
    "from datasets import generate_x_y_data_v1, generate_x_y_data_v2, generate_x_y_data_v3 \n",
    "\n",
    "# Ici, on donne un nouveau nom à une fonction utilisée partout plus bas selon l'exercice choisi. \n",
    "if exercice == 1:\n",
    "    generate_x_y_data = generate_x_y_data_v1\n",
    "if exercice == 2:\n",
    "    generate_x_y_data = generate_x_y_data_v2\n",
    "if exercice == 3:\n",
    "    generate_x_y_data = generate_x_y_data_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour le iPython notebook, les graphiques\n",
    "# sont visibles à l'intérieur. \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres du réseau de neuronne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du jeux de données pour un X et un Y : \n",
      "(50, 1, 1)\n",
      "(50, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sample_x, sample_y = generate_x_y_data(isTrain=True, batch_size=3)\n",
    "# print \"Dimensions du jeux de données pour trois X et trois Y : \"\n",
    "# print sample_x.shape\n",
    "# print sample_y.shape\n",
    "# print \"(seq_length, batch_size, output_dim)\"\n",
    "\n",
    "# # Neural net's parameters\n",
    "# seq_length = sample_x.shape[0]  # Inputs and outputs are sequences of same lenght\n",
    "# batch_size = 5  # Keeping it simple for now\n",
    "\n",
    "# # Each unit in the sequence is a float32 vector of lenght 10:\n",
    "# # Same dimension sizes just for simplicity now\n",
    "# output_dim = input_dim = sample_x.shape[-1]\n",
    "# hidden_dim = 12\n",
    "# layers_stacked_count = 2\n",
    "\n",
    "# # Optmizer: \n",
    "# learning_rate = 0.007  # Small lr to avoid problem\n",
    "# nb_iters = 100  # Crank up the iters in consequence\n",
    "# lr_decay = 0.92  # 0.9 default\n",
    "# momentum = 0.5  # 0.0 default\n",
    "# lambda_l2_reg = 0.003\n",
    "\n",
    "\n",
    "sample_x, sample_y = generate_x_y_data(isTrain=True, batch_size=1)\n",
    "print \"Dimensions du jeux de données pour un X et un Y : \"\n",
    "# (seq_length, batch_size, output_dim)\n",
    "print sample_x.shape\n",
    "print sample_y.shape\n",
    "\n",
    "# Neural net's parameters\n",
    "seq_length = sample_x.shape[0]  # Inputs and outputs are sequences of same lenght\n",
    "batch_size = 50  # Keeping it simple for now\n",
    "\n",
    "# Each unit in the sequence is a float32 vector of lenght 10:\n",
    "# Same dimension sizes just for simplicity now\n",
    "output_dim = input_dim = sample_x.shape[-1]\n",
    "hidden_dim = 35\n",
    "layers_stacked_count = 2\n",
    "\n",
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr to avoid problem\n",
    "nb_iters = 200  # Crank up the iters in consequence\n",
    "lr_decay = 0.92  # 0.9 default\n",
    "momentum = 0.5  # 0.0 default\n",
    "lambda_l2_reg = 0.003\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Définition de l'architecture neuronale à la seq2seq\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/basic_seq2seq.png\" />\n",
    "\n",
    "Dans notre cas, comparativement à dans cette image, nous avons en entrée des données de signaux. Aussi, nous n'avons pas une boucle de feedback pour le décodeur: Notre décodeur a en entrée seulement le symbole \\<GO\\> suivi de la dernière valeurs de signal envoyées à l'encodeur, à répétition. Cela pourrait être modifié afin d'inclure proprement un feedback des prédictions dans la prochaine entrée dans le temps. \n",
    "\n",
    "Spécialement dans notre cas aussi, afin de simplifier le modèle, nous avons la même cellule récurrente pour l'encodeur et le décodeur, plutôt qu'une cellule différente indépendante, ce qui est plus simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "# sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.variable_scope('Seq2seq'):\n",
    "\n",
    "    # Entrées dans l'encodeur\n",
    "    enc_inp = [\n",
    "        tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "           for t in range(seq_length)\n",
    "    ]\n",
    "\n",
    "    # Sorties attendues du décodeur\n",
    "    expected_sparse_output = [\n",
    "        tf.placeholder(tf.float32, shape=(None, output_dim), name=\"expected_sparse_output_\".format(t))\n",
    "          for t in range(seq_length)\n",
    "    ]\n",
    "    \n",
    "    # Donner un symbole de départ \"GO\" au décodeur, et toujours la derniere valeur de l'encodeur\n",
    "    dec_inp = [ tf.zeros_like(enc_inp[0], dtype=np.float32, name=\"GO\") ] + enc_inp[:-1]\n",
    "\n",
    "    # Créer un nombre `layers_stacked_count` de RNN empilés\n",
    "    cells = []\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            # GRU est similaire au LSTM, \n",
    "            # mais pas exactement pareil à l'intérieur: \n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    # Adapter les dimensions des entrees et sorties pour le RNN seq2seq: \n",
    "    w_in = tf.Variable(tf.random_normal([input_dim, hidden_dim]))\n",
    "    b_in = tf.Variable(tf.random_normal([hidden_dim], mean=1.0))\n",
    "    w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "    b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "    \n",
    "    reshaped_inputs = [tf.nn.relu(tf.matmul(i, w_in) + b_in) for i in enc_inp]\n",
    "    \n",
    "    # Ici, l'encodeur et le décodeur\n",
    "    # utilisent les mêmes poids pour la cellule: \n",
    "    dec_outputs, dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(\n",
    "        enc_inp, \n",
    "        dec_inp, \n",
    "        cell\n",
    "    )\n",
    "    \n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\")\n",
    "    # Sortie finale du réseau de neurones, avec une transformation linéaire:\n",
    "    reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    # L2 loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, expected_sparse_output):\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "        \n",
    "    # L2 regularization (pour éviter l'overfitting et pour mieux généraliser)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "            \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/200, loss: 360.647216797\n",
      "Step 10/200, loss: 204.846786499\n",
      "Step 20/200, loss: 196.755813599\n",
      "Step 30/200, loss: 201.731918335\n",
      "Step 40/200, loss: 201.222991943\n",
      "Step 50/200, loss: 205.339355469\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Training step: we optimize for every outputs Y at once, \n",
    "    feeding all inputs X\n",
    "    \n",
    "    I do not know yet how to deal with \n",
    "    the enc_inp tensor declared earlier\n",
    "    \"\"\"\n",
    "    X, Y = generate_x_y_data(isTrain=True, batch_size=batch_size)\n",
    "\n",
    "    feed_dict = {\n",
    "        enc_inp[t]: X[t] for t in range(len(enc_inp))\n",
    "    }\n",
    "    feed_dict.update({expected_sparse_output[t]: Y[t] for t in range(len(expected_sparse_output))})\n",
    "\n",
    "    _, loss_t = sess.run([train_op, loss], feed_dict)\n",
    "    return loss_t\n",
    "\n",
    "\n",
    "# Train\n",
    "train_losses = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in range(nb_iters+1):\n",
    "    loss_t = train_batch(batch_size)\n",
    "    train_losses.append(loss_t)\n",
    "    if t % 10 == 0: \n",
    "        print \"Step {}/{}, loss: {}\".format(t, nb_iters, loss_t)\n",
    "\n",
    "print \"Final loss: {}\".format(loss_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(np.log(train_losses), label=\"Loss\")\n",
    "plt.title(\"Erreurs de l'entrainement au cours du temps (sur une echelle logarithmique)\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('log(Loss)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tester\n",
    "nb_predictions = 5\n",
    "print \"Visualisons {} prédictions quelconques avec nos signaux :\".format(nb_predictions)\n",
    "\n",
    "X, Y = generate_x_y_data(isTrain=True, batch_size=nb_predictions)\n",
    "feed_dict = {enc_inp[t]: X[t] for t in range(seq_length)}\n",
    "outputs = sess.run([reshaped_outputs], feed_dict)\n",
    "\n",
    "for j in range(nb_predictions): \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    \n",
    "    for k in range(output_dim):\n",
    "        past = X[:,j,k]\n",
    "        expected = Y[:,j,k]\n",
    "        pred = np.array(outputs[0])[:,j,k]\n",
    "        \n",
    "        label1 = \"Valeurs precedentes\" if k==0 else \"_nolegend_\"\n",
    "        label2 = \"Vraie valeurs futures\" if k==0 else \"_nolegend_\"\n",
    "        label3 = \"Predictions\" if k==0 else \"_nolegend_\"\n",
    "        plt.plot(range(len(past)), past, \"o--b\", label=label1)\n",
    "        plt.plot(range(len(past), len(expected)+len(past)), expected, \"x--b\", label=label2)\n",
    "        plt.plot(range(len(past), len(pred)+len(past)), pred, \"o--y\", label=label3)\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Predictions v.s. vraies valeurs\")\n",
    "    plt.show()\n",
    "\n",
    "print \"Le signal peut contenir plusieurs dimensions en sortie à la fois.\"\n",
    "print \"Si c'est le cas, les signaux sont empilés avec la même couleur.\"\n",
    "print \"En vrai, on pourrait imaginer plusieurs symboles boursiers évoluant dans le\"\n",
    "print \"même référentiel de temps, analysés en même temps par le même réseau de neurones.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
